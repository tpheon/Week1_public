{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Nearest_Neighbors_Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xb0q8y0rq6Ve"
      },
      "source": [
        "# Nearest Neighbors Algorithms\n",
        "\n",
        "In this notebook we will describe Nearest Neighbors Algorithms and look at implementations using TODO\n",
        "\n",
        "The basic concept for **Nearest Neighbors Algorithms** (NNAs) is to classify a datum by finding one or more points that have similar features. The most similar points are called the nearest neighbors. Once found, the input datum can be put in the same class as its neighbors. We can also use this to predict the value of missing features for the input datum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdwUJygSGU9F",
        "colab_type": "text"
      },
      "source": [
        "## k-Nearest Neighbors\n",
        "\n",
        "The most commonly used NNA is **k-Nearest Neighbors,** in which the top $k$ nearest neighbors (best matches) are identified. In most instantiations of k-NNA, classification or prediction is based on a *majority vote* of the $k$ nearest neighbors. For example, if $k = 5$, and at least 3 out of the 5 nearest neighbors to an input datum are class A, then we would assign the new datum to class A.\n",
        "\n",
        "For a more complex example, see the image below. Here, if $k = 1$, the green circle would be assigned to Class 1, since the nearest point is a blue square. However, if $k = 3$ the answer becomes Class 2, since the next two closest are both red triangles.\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*Sk18h9op6uK9EpT8.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZrldzQgy9ud",
        "colab_type": "text"
      },
      "source": [
        "## k-NNA with sklearn\n",
        "\n",
        "In this section, we will use sklearn to build a k-NNA model for a data set describing cars. Our goal will be to classify the cars into one of two categories: \"cool\" or \"uncool.\" Clearly these are subjective terms, but that's ok: we will provide a manually classified training set.\n",
        "\n",
        "For example, if we consider the variables *horsepower, number of seats,* and *manual (0) or automatic (1)*, our manually classified training set might look like this:\n",
        "\n",
        "*   150, 5, 0, uncool (2008 Honda Civic)  \n",
        "*   320, 5, 0, cool (2011 Dodge Charger)\n",
        "*   383, 3, 1, cool (1985 Chevy Blazer)\n",
        "*   210, 7, 0, uncool (2001 Honda Odyssey)\n",
        "\n",
        "Let's say we're trying to predict whether the 2017 Bugatti Veyron (1500hp, 2 seats, manual: 1) is cool or not. Our first step is to load the data into a python structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4VMsygjy9ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e45a264a-29e9-43d4-d322-78c76841ae03"
      },
      "source": [
        "# -- imports -- #\n",
        "import numpy as np\n",
        "import pandas as po\n",
        "\n",
        "# -- loading the data -- #\n",
        "cars_dict = {'2008 Honda Civic':    {'hp':150., 'seats':5., 'auto':0., 'cool':0}, \n",
        "             '2011 Dodge Charger' : {'hp':320., 'seats':5., 'auto':0., 'cool':1}, \n",
        "             '1985 Chevy Blazer':   {'hp':383., 'seats':3., 'auto':1., 'cool':1}, \n",
        "             '2001 Honda Odyssey':  {'hp':210., 'seats':7., 'auto':0., 'cool':0}, \n",
        "             '2017 Bugatti Veyron': {'hp':1500.,'seats':2., 'auto':1., 'cool':None}}\n",
        "\n",
        "data = po.DataFrame.from_dict(cars_dict,orient='index')\n",
        "data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hp</th>\n",
              "      <th>seats</th>\n",
              "      <th>auto</th>\n",
              "      <th>cool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008 Honda Civic</th>\n",
              "      <td>150.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011 Dodge Charger</th>\n",
              "      <td>320.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985 Chevy Blazer</th>\n",
              "      <td>383.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001 Honda Odyssey</th>\n",
              "      <td>210.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017 Bugatti Veyron</th>\n",
              "      <td>1500.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         hp  seats  auto  cool\n",
              "2008 Honda Civic      150.0    5.0   0.0   0.0\n",
              "2011 Dodge Charger    320.0    5.0   0.0   1.0\n",
              "1985 Chevy Blazer     383.0    3.0   1.0   1.0\n",
              "2001 Honda Odyssey    210.0    7.0   0.0   0.0\n",
              "2017 Bugatti Veyron  1500.0    2.0   1.0   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5STDuS1DF0PA"
      },
      "source": [
        "To determine the class of our Bugatti using NNA, we need to measure its \"nearness\" to other cars. One of the simplest metrics for this is Euclidian distance, defined as the square root of the sum of the difference between each feature squared:\n",
        "\n",
        "![alt text](https://i.stack.imgur.com/2y0bx.png.)\n",
        "\n",
        "Some other approaches include Chi square distance and cosine distance. For further reading on distance functions, see this article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/.\n",
        "\n",
        "Note that by using Euclidian distance, we are normalizing the data in a way that assumes all features are equally important. One consequence of this is that values that are generally larger (such as horsepower) will end up having a larger impact on the distance. Unless we have some reason to believe that the larger features are more important, we will want to balance features by **normalizing the data.**\n",
        "\n",
        "\n",
        "One quick and easy way to normalize data is to divide each datum by the maximum value in its category. Let's do that for our data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aBhWCPsFMn-C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "48fe2f59-9e6d-4613-b3a0-3bbf9f6466c3"
      },
      "source": [
        "# Normalizing the data by dividing each value by the maximum value in its row. \n",
        "# Remember that we don't normalize the label ()\n",
        "\n",
        "data_norm_divide = data\n",
        "\n",
        "for i in ['hp','seats','auto']:\n",
        "    data_norm_divide[i] = data[i]/max(data[i].values)\n",
        "    \n",
        "data_norm_divide"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hp</th>\n",
              "      <th>seats</th>\n",
              "      <th>auto</th>\n",
              "      <th>cool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008 Honda Civic</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011 Dodge Charger</th>\n",
              "      <td>0.213333</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985 Chevy Blazer</th>\n",
              "      <td>0.255333</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001 Honda Odyssey</th>\n",
              "      <td>0.140000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017 Bugatti Veyron</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           hp     seats  auto  cool\n",
              "2008 Honda Civic     0.100000  0.714286   0.0   0.0\n",
              "2011 Dodge Charger   0.213333  0.714286   0.0   1.0\n",
              "1985 Chevy Blazer    0.255333  0.428571   1.0   1.0\n",
              "2001 Honda Odyssey   0.140000  1.000000   0.0   0.0\n",
              "2017 Bugatti Veyron  1.000000  0.285714   1.0   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXcfmnfzy9u0",
        "colab_type": "text"
      },
      "source": [
        "As usual, a tool already exists to do this. Using sklearn, we can normalize data with an object called a StandardScaler. We will use this in the example below, but feel free to also read the documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html.\n",
        "\n",
        "Normalization, or more generally \"standardization,\" is a common requirement for machine learning estimators. Beyond shifting values to similar magnitudes, many estimators will run into trouble if the values within individual features are not roughly normally distributed. By **normal distribution,** we mean a Gaussian distribution with 0 mean and unit variance. The sklearn StandardScaler will attempt to scale all the data to approximate a normal distribution for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6hK0zn1y9u2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9c6c556c-2a9d-454f-8e63-5f97180560e1"
      },
      "source": [
        "# Normalize data by removing the mean and scaling to unit variance from each feature\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "data_unitnorm = po.DataFrame.from_dict(cars_dict,orient='index')\n",
        "\n",
        "for i in ['hp','seats','auto']:\n",
        "    feature_data = data_unitnorm[i].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data)\n",
        "    data_unitnorm[i] = scaler.transform(feature_data)\n",
        "    \n",
        "data_unitnorm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hp</th>\n",
              "      <th>seats</th>\n",
              "      <th>auto</th>\n",
              "      <th>cool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2008 Honda Civic</th>\n",
              "      <td>-0.724651</td>\n",
              "      <td>0.344124</td>\n",
              "      <td>-0.816497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2011 Dodge Charger</th>\n",
              "      <td>-0.384908</td>\n",
              "      <td>0.344124</td>\n",
              "      <td>-0.816497</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1985 Chevy Blazer</th>\n",
              "      <td>-0.259004</td>\n",
              "      <td>-0.802955</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001 Honda Odyssey</th>\n",
              "      <td>-0.604742</td>\n",
              "      <td>1.491202</td>\n",
              "      <td>-0.816497</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017 Bugatti Veyron</th>\n",
              "      <td>1.973305</td>\n",
              "      <td>-1.376494</td>\n",
              "      <td>1.224745</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           hp     seats      auto  cool\n",
              "2008 Honda Civic    -0.724651  0.344124 -0.816497   0.0\n",
              "2011 Dodge Charger  -0.384908  0.344124 -0.816497   1.0\n",
              "1985 Chevy Blazer   -0.259004 -0.802955  1.224745   1.0\n",
              "2001 Honda Odyssey  -0.604742  1.491202 -0.816497   0.0\n",
              "2017 Bugatti Veyron  1.973305 -1.376494  1.224745   NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6dLhGxay9u7",
        "colab_type": "text"
      },
      "source": [
        "Now that the data is standardized, we can begin building our NNA algorithm. First, we will need a function that calculates the Euclidean distance between two data points. For our purposes, we will assume the data points will be stored in arrays of the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2rayoFQnOyvY",
        "colab": {}
      },
      "source": [
        "# Distance function using formula for euclidean distance\n",
        "\n",
        "def euclidean_dist(datum1, datum2):\n",
        "    inner_val = 0.0\n",
        "    \n",
        "    for g in range(datum1.shape[0]):\n",
        "        inner_val += (datum1[g]- datum2[g]) ** 2\n",
        "    \n",
        "    distance = math.sqrt(inner_val)\n",
        "    return(distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6IaktG5_mUNf"
      },
      "source": [
        "Next, we calculate the distance between our Bugatti and each other car using the *euclidean_dist* function. For the sake of testing, we do this twice: once for the normalized data (*data_norm_divide*), and once for the data standardized by sklearn (*data_unitnorm*). Note that we will not input the classification column of cool/uncool to our distance function, which means taking a subset of each array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rY0Wuzz9LZzu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "15bbbefb-d5e7-4ff4-92f6-645e58f8336e"
      },
      "source": [
        "# FYI: This is how you can call a specific row by name and sub-select features\n",
        "bugatti = data.loc[\"2017 Bugatti Veyron\"][[\"hp\",\"seats\",\"auto\"]].values\n",
        "\n",
        "import math\n",
        "\n",
        "# -- normalized data by dividing -- #\n",
        "print('Euclidean Distances to 2017 Bugatti Veyron (V1)')\n",
        "for car in range(len(data_norm_divide)):\n",
        "    d = euclidean_dist(data_norm_divide.iloc[4,:3].values, \n",
        "                       data_norm_divide.iloc[car, :3].values)\n",
        "    print('  {}: \\t{:01.3f}'.format(car,d))\n",
        "\n",
        "# -- standardized data -- #\n",
        "print('\\nEuclidean Distances to 2017 Bugatti Veyron (V2)')\n",
        "for car in range(len(data_unitnorm)):\n",
        "    d = euclidean_dist(data_unitnorm.iloc[4,:3].values, \n",
        "                       data_unitnorm.iloc[car, :3].values)\n",
        "    print('  {}: \\t{:01.3f}'.format(car,d))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Euclidean Distances to 2017 Bugatti Veyron (V1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-de6f6dcd5529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Euclidean Distances to 2017 Bugatti Veyron (V1)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_norm_divide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     d = euclidean_dist(data_norm_divide.iloc[4,:3].values, \n\u001b[0m\u001b[1;32m     10\u001b[0m                        data_norm_divide.iloc[car, :3].values)\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  {}: \\t{:01.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'euclidean_dist' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rHRn4t68nIbB"
      },
      "source": [
        "For the normalized data (with each feature divided by its maximum value), we get the following distances:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 0.758\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 1.500\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 1.412\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 1.343\n",
        "\n",
        "For the standardized data (using sklearn's StandardScaler), we get the following distances:\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Blazer = 2.305\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Odyssey = 4.363\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Civic = 3.796\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bugatti - Charger = 3.562\n",
        "\n",
        "Notice that both techniques yielded the same order of cars from nearest to farthest. This is coincidental, and unlikely to happen with larger or more varied data sets.\n",
        "\n",
        "Since the distance between the Bugatti and Blazer is the smallest, if $k = 1$, we would classify the Bugatti as cool. However, if $k = 4$ there would no longer be a majority, and we would not be able to classify the Bugatti in either category without a tiebreaker protocol.\n",
        "\n",
        "Generally speaking, larger values of $k$ reduce noise, but also make the boundaries between classes less distinct. The best value of $k$ will vary by data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vo5iwH6soijh"
      },
      "source": [
        "# Example kNN TODO : Move to problem set\n",
        "\n",
        "Next we will see if we can use k-NNA on the Pima Indian Diabetes dataset.\n",
        "\n",
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hQAjk52WQ1da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "14b27fa3-9a09-4762-aa2e-bcb576400d6c"
      },
      "source": [
        "# -- loading dataset -- #\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "data = po.read_csv(url, names=names)\n",
        "\n",
        "# -- dropping NaN rows -- #\n",
        "invalid = ['plas', 'pres', 'skin', 'test', 'mass']\n",
        "\n",
        "for i in invalid:\n",
        "    data[i].replace(to_replace=0, value=np.nan, inplace=True)\n",
        "    \n",
        "data = data.dropna(axis=0).reset_index(drop=True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preg</th>\n",
              "      <th>plas</th>\n",
              "      <th>pres</th>\n",
              "      <th>skin</th>\n",
              "      <th>test</th>\n",
              "      <th>mass</th>\n",
              "      <th>pedi</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>89.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>78.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.248</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>197.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>543.0</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.158</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>189.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>846.0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.398</td>\n",
              "      <td>59</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   preg   plas  pres  skin   test  mass   pedi  age  class\n",
              "0     1   89.0  66.0  23.0   94.0  28.1  0.167   21      0\n",
              "1     0  137.0  40.0  35.0  168.0  43.1  2.288   33      1\n",
              "2     3   78.0  50.0  32.0   88.0  31.0  0.248   26      1\n",
              "3     2  197.0  70.0  45.0  543.0  30.5  0.158   53      1\n",
              "4     1  189.0  60.0  23.0  846.0  30.1  0.398   59      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lO_knzAAe93n"
      },
      "source": [
        "Now, let's clearly define which columns will act as explanatory variables, and which column will be the target value, and split the dataset between your training data and testing data.  Let's try an 80-20 split and use sklearn's [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method (set random_state = 0 so we get the same output each time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8pRTCC3PatJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47ceb8c1-e308-4974-ba9f-91587232c917"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# columns we will use to make predictions with (features!) feel free to play around with these\n",
        "x_cols = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "# column that we want to predict\n",
        "y_col = 'class'\n",
        "\n",
        "\n",
        "# 80-20 train-test split of datset\n",
        "test_size = 0.2\n",
        "x_training, x_testing, y_training, y_testing = train_test_split(data[x_cols], data[y_col], test_size=test_size, random_state=0)\n",
        "\n",
        "print(x_training.shape, y_training.shape)\n",
        "print(x_testing.shape, y_testing.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(313, 8) (313,)\n",
            "(79, 8) (79,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrnVJ8Ovy9vT",
        "colab_type": "text"
      },
      "source": [
        "## Normalizing Data\n",
        "\n",
        "Let's not forget to normalize the data! We'll use sklearn's StandardScaler normalization like we did before to normalize the training **and** testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt29VKMJy9vT",
        "colab_type": "code",
        "colab": {},
        "outputId": "900e8d17-5e40-4ce4-840d-c4aa910893e9"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "for i in list(x_training):\n",
        "    feature_data_train = x_training[i].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_train)\n",
        "    x_training[i] = scaler.transform(feature_data_train)\n",
        "    \n",
        "for j in list(x_testing):\n",
        "    feature_data_test = x_testing[j].values.reshape(-1, 1)\n",
        "    scaler.fit(feature_data_test)\n",
        "    x_testing[j] = scaler.transform(feature_data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "C:\\Users\\emily\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEWkS1gCy9vY",
        "colab_type": "text"
      },
      "source": [
        "## Writing our k-NNA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzZQ1VhiPSJU"
      },
      "source": [
        "The predict method that we'll make below needs to \n",
        "1. Compute the euclidean distance between the “new” observation and all the data points in the training set. \n",
        "2. Assign the corresponding label to the observation\n",
        "3. Select the k nearest ones and perform a \"majority vote\"\n",
        "\n",
        "This is one way to code the nearest neighbor algorithm from scratch. Go through and try to understand (at a high level at least) what each line is doing, then run the cells. The predictions for each test sample will be the output, as well as how long it took your machine to run the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UCganwZfe9LA",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def predict(x_training, y_training, x_test_sample, k):\n",
        "    \n",
        "    # create list for distances and targets\n",
        "    distances = []\n",
        "    targets = []\n",
        "\n",
        "    # use our previously made euclidean distance function, calculate distance to all samples in training set\n",
        "    for q in list(x_training.index):\n",
        "        distances.append([euclidean_dist(x_test_sample, x_training.loc[q]), q])\n",
        "  \n",
        "    # sort distances (smallest to largest)\n",
        "    distances = sorted(distances)\n",
        "  \n",
        "    # make a list of the k neighbors' targets\n",
        "    for i in range(k):\n",
        "        dix = distances[i][1]\n",
        "        targets.append(y_training.loc[dix])\n",
        "        \n",
        "    # retrieve the most common target\n",
        "    c = Counter(targets)\n",
        "    \n",
        "    return c.most_common()[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwwEq3RHy9ve",
        "colab_type": "text"
      },
      "source": [
        "And now we'll write our k-NNA!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNA504XOvjvY",
        "colab": {}
      },
      "source": [
        "def knn(x_training, y_training, x_testing, k):\n",
        "    \n",
        "    predictions = np.empty([x_testing.shape[0],])\n",
        "    \n",
        "    # loop over all test samples\n",
        "    px = 0\n",
        "    for i in list(x_testing.index):\n",
        "        predictions[px] = predict(x_training, y_training, x_testing.loc[i], k)\n",
        "        px+=1\n",
        "        \n",
        "    return predictions.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iew5gW_AbnfN",
        "colab": {},
        "outputId": "970fbd71-3b6e-498f-bc73-9499eb2a9dc0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "predictions_slow = knn(x_training, y_training, x_testing, k=5)\n",
        "\n",
        "print('Took {} seconds'.format(time.time() - start))\n",
        "print(\"Testing Accuracy is \", accuracy_score(y_testing,predictions_slow)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 6.040954828262329 seconds\n",
            "Testing Accuracy is  78.48101265822784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7-2ZM8AGIe0S"
      },
      "source": [
        "## Using sklearn's k-NNA\n",
        "\n",
        "Luckily for us, sklearn has some quick and easy functions for normalizing the data, finding Euclidean distances, training, and testing with [k-NNA](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). Try k = 5 to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R0FsKkp_qw-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13073
        },
        "outputId": "39c27885-10d9-4f03-9bff-ac7c41d5dcb8"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# creating model with sklearn's KNeighborsClassifier -- after running these cells play around with the parameter n!\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# training/fitting a model with training data\n",
        "knn.fit(x_training, y_training)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "           weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8y1RuAoy9vr",
        "colab_type": "code",
        "colab": {},
        "outputId": "e203abae-7fcc-4314-d277-b4febb322b55"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_train_pred=knn.predict(x_training)\n",
        "start = time.time()\n",
        "predictions_fast = knn.predict(x_testing)\n",
        "\n",
        "print('Took {} seconds'.format(time.time() - start))\n",
        "print(\"Training Accuracy is \", accuracy_score(y_training, y_train_pred)*100)\n",
        "print(\"Testing Accuracy is \", accuracy_score(y_testing,predictions_fast)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 0.0049877166748046875 seconds\n",
            "Training Accuracy is  81.15015974440894\n",
            "Testing Accuracy is  78.48101265822784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMjhYcamy9vv",
        "colab_type": "text"
      },
      "source": [
        "Check sklearn's predictions on testing data and make sure they match yours.  Sklearn is faster, but you should get the same answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-hHcKgsy9vw",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions and Limitations\n",
        "\n",
        "k-NNA provides a good baseline classifier for machine learning that is conceptually intuitive and easy to implement and train. One major advantage is that, by standardizing input data, k-NNA can combine any number of features regardless of their original distributions.\n",
        "\n",
        "However, k-NNA can run into problems with more complex data sets:\n",
        "* With additional dimensions, it can be harder to define meaningful distances.\n",
        "* The testing phase can slow down significantly with large data sets since each point's distance is measured to every other point.\n",
        "* \"Majority votes\" may be skewed if one classification is significantly more common than the others.\n",
        "* There is no consideration for correlated features.\n",
        "\n",
        "As a final note, there are other ways to instantiate k-NNA's, and other types of NNA beyond k-Nearest. For example, to solve the large majority problem, we could have used a weighted voting system where nearer neighbors' votes carry more weight."
      ]
    }
  ]
}